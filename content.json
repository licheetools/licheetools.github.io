{"meta":{"title":"编程思录，记录思考","subtitle":null,"description":null,"author":"Envy","url":"http://licheetools.github.io","root":"/"},"pages":[{"title":"分类","date":"2017-02-22T00:39:04.000Z","updated":"2019-04-13T08:39:04.005Z","comments":false,"path":"categories/index.html","permalink":"http://licheetools.github.io/categories/index.html","excerpt":"","text":""},{"title":"font","date":"2019-04-13T01:37:12.000Z","updated":"2019-04-13T01:37:14.951Z","comments":true,"path":"categories/font/index.html","permalink":"http://licheetools.github.io/categories/font/index.html","excerpt":"","text":""},{"title":"关于博主","date":"2017-02-22T00:39:04.000Z","updated":"2019-04-13T07:44:35.475Z","comments":true,"path":"categories/about/index.html","permalink":"http://licheetools.github.io/categories/about/index.html","excerpt":"","text":""},{"title":"java","date":"2019-04-13T01:38:05.000Z","updated":"2019-04-13T01:38:06.242Z","comments":true,"path":"categories/java/index.html","permalink":"http://licheetools.github.io/categories/java/index.html","excerpt":"","text":""},{"title":"data","date":"2019-04-13T08:44:34.000Z","updated":"2019-04-13T08:44:34.957Z","comments":true,"path":"categories/data/index.html","permalink":"http://licheetools.github.io/categories/data/index.html","excerpt":"","text":""},{"title":"你有什么话，想对我说嘛？","date":"2017-03-03T01:29:53.000Z","updated":"2019-04-13T08:12:40.356Z","comments":true,"path":"categories/liuyan/index.html","permalink":"http://licheetools.github.io/categories/liuyan/index.html","excerpt":"","text":""},{"title":"life","date":"2019-04-13T01:43:45.000Z","updated":"2019-04-13T01:43:48.121Z","comments":true,"path":"categories/life/index.html","permalink":"http://licheetools.github.io/categories/life/index.html","excerpt":"","text":""},{"title":"spider","date":"2019-04-13T01:38:39.000Z","updated":"2019-04-13T01:38:39.430Z","comments":true,"path":"categories/spider/index.html","permalink":"http://licheetools.github.io/categories/spider/index.html","excerpt":"","text":""},{"title":"付费内容","date":"2019-04-13T01:38:53.000Z","updated":"2019-04-13T03:12:55.310Z","comments":true,"path":"categories/money/index.html","permalink":"http://licheetools.github.io/categories/money/index.html","excerpt":"","text":""},{"title":"flask","date":"2019-04-13T01:38:26.000Z","updated":"2019-04-13T01:38:26.750Z","comments":true,"path":"categories/flask/index.html","permalink":"http://licheetools.github.io/categories/flask/index.html","excerpt":"","text":""},{"title":"django","date":"2019-04-13T01:38:16.000Z","updated":"2019-04-13T01:38:16.284Z","comments":true,"path":"categories/django/index.html","permalink":"http://licheetools.github.io/categories/django/index.html","excerpt":"","text":""}],"posts":[{"title":"关于博主","slug":"关于博主","date":"2017-03-04T08:27:48.000Z","updated":"2019-04-13T08:46:42.925Z","comments":true,"path":"2017/03/04/关于博主/","link":"","permalink":"http://licheetools.github.io/2017/03/04/关于博主/","excerpt":"","text":"我，一个正在进击的技术控，相信越努力，越自由。非常高兴能和大家在这里相遇。这是我的个人博客，搭建的初衷是广交朋友，广学知识。那么你肯定觉得我是一个技术大牛（哈哈），其实我只是一个技术控，对那些有趣的技术非常好奇罢了，离真正的大神还差一个”地中海”式的发型。目前主要从事前端工作，但是非常喜欢java， python， kotlin等编程语言，所以你如果也有此爱好的话，我们不妨交流一下学习心得。今后我会在这个博客上更新一些有趣的技术和分享我学习，工作，生活上的点滴瞬间。你不努力一下，你就体会不到被人追赶的滋味.生活不只有Coding还有诗和远方~~~喜欢旅游，看电影，偶尔还玩会游戏（最近在玩王者荣耀，尽管队友坑，但我也坑。。。）对了，有一些技术上的问题可以给我发微信消息（事情多，不能立即回复，甚至忘记回复，还请不要生气）。我在很多社区都有账号，需要联系我的朋友可以关注并加好友呃…… 邮箱: 2810706745@qq.com QQ: 2810706745 微信: ZY20160801 新浪微博: 点击这里 GitHub: 点击这里 声明： 1.本博客文章只是为了个人学习以及在一些技术型社区发表，不做商业使用; 2.如其中引用到其他文章经典的部分，本人大部分都会加上引用或者在文章底部的参考文献中注明，如果不慎引用了你的文章但是却没有注明的请联系我; 3.博客内文章欢迎转载，但转载前请联系我，保留出处和作者，不做商业使用，谢谢~~~","categories":[{"name":"about","slug":"about","permalink":"http://licheetools.github.io/categories/about/"}],"tags":[{"name":"关于博主","slug":"关于博主","permalink":"http://licheetools.github.io/tags/关于博主/"}]},{"title":"你有什么话，想对我说嘛？","slug":"你有什么话，想对我说嘛？","date":"2017-03-03T01:29:53.000Z","updated":"2019-04-13T08:25:39.838Z","comments":true,"path":"2017/03/03/你有什么话，想对我说嘛？/","link":"","permalink":"http://licheetools.github.io/2017/03/03/你有什么话，想对我说嘛？/","excerpt":"","text":"","categories":[{"name":"liuyan","slug":"liuyan","permalink":"http://licheetools.github.io/categories/liuyan/"}],"tags":[{"name":"留言板","slug":"留言板","permalink":"http://licheetools.github.io/tags/留言板/"}]},{"title":"爬虫实战系列：（二）猫途鹰(tripadvisor)旅游景点信息爬取","slug":"爬虫实战系列：（二）猫途鹰(tripadvisor)旅游景点信息爬取","date":"2017-03-02T11:09:47.000Z","updated":"2019-04-13T08:16:34.714Z","comments":true,"path":"2017/03/02/爬虫实战系列：（二）猫途鹰(tripadvisor)旅游景点信息爬取/","link":"","permalink":"http://licheetools.github.io/2017/03/02/爬虫实战系列：（二）猫途鹰(tripadvisor)旅游景点信息爬取/","excerpt":"","text":"说在前面在上一篇中，我们说到某个内容从属于多个分类(即它有多个标签)，应该如何把它所属的全部标签都爬取下来呢？这一期博主就将教你如何实现这一功能。我们以猫途鹰这个网站为例，向大家演示一下。 爬取内容如下图所示，我们需要爬取该图中所示景点的信息，具体包括：标题，封面图片，分类等信息。 知识补充get()方法的使用我们在上一篇中，获取标题，地址，单标签的文本信息采用了get_text()方法，但是如果要获取图片呢？我们是不能和之前一样，要采用一种新的方法：get(‘attr’)。这么说你可能有些困惑，举个简单的例子，看下面的代码:&lt;a href=http://licheetools.top&gt; share my life with you &lt;/a&gt;如果你只想获取网站信息，只需要.get(‘href’)即可。获取图片只需要.get(‘src’)因为图片的格式一般是这样的：&lt;img src=&quot;http://licheetools.top/index.html/first.jpg&quot; alt=&quot;第一张图片&quot;&gt; stripped_strings的用法在这里你可以认为是高级的get_text(),它这个可以同时取出多个文本，待会就采用这个来爬取多个标签。 字符串的格式化字符串的格式化通常采用{}和:来代替传统的%方式，具体如下：1、使用位置参数从下面的例子可以看出位置参数不受顺序约束，且可以为{},只要format里有相对应的参数值即可,参数索引从0开，传入位置参数列表可用*列表；12345678910111213141516a='my name is &#123;&#125; ,age &#123;&#125;'.format('joe',18)print(a)my name is joe ,age 18b='my name is &#123;1&#125; ,age &#123;0&#125;'.format(10,'joe')print(b)my name is joe ,age 10c='my name is &#123;1&#125; ,age &#123;0&#125; &#123;1&#125;'.format(10,'joe')print(c)my name is joe ,age 10 joedata=['joe',18]d='my name is &#123;&#125; ,age &#123;&#125;'.format(*data)print(d)my name is joe ,age 18 2、使用关键字参数采用字典当关键字参数传入值，不过关键字参数值要对得上，可以在字典前加**12345678e='my name is &#123;name&#125;,age is &#123;age&#125;'.format(name='joe',age=19)print(e)my name is joe,age is 19data=&#123;'name':'joe','age':19&#125;f='my name is &#123;name&#125;,age is &#123;age&#125;'.format(**data)print(f)my name is joe,age is 19 其他参数我在这里就不介绍了，你可以看这篇文章python之字符串格式化(formate) 代码内容123456789101112131415161718192021222324252627282930313233#！/user/bin/pytthon# -*- coding:utf-8 -*-# @Time: 2017/04/15 20:15# @Author: lichexo# @File: test_05.pyfrom bs4 import BeautifulSoupimport requestsimport timeurl='https://www.tripadvisor.cn/Attractions-g60763-Activities-New_York_City_New_York.html'urls=['https://www.tripadvisor.cn/Attractions-g60763-Activities-oa&#123;&#125;-New_York_City_New_York.html#FILTERED_LIST'.format(str(i)) for i in range(30,1140,30)]def get_attraction(url,data=None): web_data = requests.get(url) time.sleep(4) soup = BeautifulSoup(web_data.text, 'lxml') titles = soup.select('div.listing_title &gt; a[target=\"_blank\"]') images = soup.select('img[width=\"180\"]') cates = soup.select('div.tag_line &gt; div') for title,image,cate in zip(titles,images,cates): data=&#123; 'title':title.get_text(), 'image':image.get('src'), 'cate':list(cate.stripped_strings), &#125; print(data)get_attraction(url)for single_url in urls: get_attraction(single_url)print(urls) 运行结果由于抓取的内容太多，这里只呈现部分信息。12345678910111213141516171819202122232425262728293031323334&#123;'title': '曼哈顿天际线', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['景点与地标']&#125;&#123;'title': '大都会艺术博物馆', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['景点与地标', ',', '艺术博物馆']&#125;&#123;'title': '中央公园', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['景点与地标', ',', '公园', ',', '景观步行区']&#125;&#123;'title': '9/11纪念馆', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['特色博物馆', ',', '景点与地标', ',', '纪念碑与雕像', ',', '古迹']&#125;&#123;'title': '峭石之巅观景台', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['观景台', ',', '瞭望台']&#125;&#123;'title': '布鲁克林大桥', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['桥梁']&#125;&#123;'title': '圣帕提克大教堂', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['教堂']&#125;&#123;'title': '百老汇', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['景点与地标']&#125;&#123;'title': '大中央车站', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['建筑', ',', '景点与地标']&#125;&#123;'title': '布莱恩公园', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['公园']&#125;&#123;'title': '纽约公共图书馆', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['特色博物馆']&#125;&#123;'title': '世贸一号观景台', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['观景台', ',', '建筑']&#125;&#123;'title': '史泰登岛渡轮', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['轮渡']&#125;&#123;'title': '高线公园', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['景观步行区', ',', '公园']&#125;&#123;'title': '帝国大厦', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['建筑', ',', '景点与地标', ',', '观景台']&#125;&#123;'title': '自由女神像', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['纪念碑与雕像', ',', '景点与地标']&#125;&#123;'title': '洛克菲勒中心', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['景点与地标', ',', '建筑']&#125;&#123;'title': '现代艺术博物馆', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['艺术博物馆']&#125;&#123;'title': 'The Oculus', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['景点与地标']&#125;&#123;'title': '无畏海空宇宙博物馆', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['剧院']&#125;&#123;'title': '阿波罗剧院', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['纪念碑与雕像', ',', '建筑']&#125;&#123;'title': '华盛顿广场公园', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['特色博物馆']&#125;&#123;'title': '圣保罗教堂', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['剧院']&#125;&#123;'title': '圣约翰教堂', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['公园', ',', '景点与地标', ',', '周边']&#125;&#123;'title': '河滨公园', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': []&#125;&#123;'title': 'Studio 54', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['教堂', ',', '圣地与宗教景点']&#125;&#123;'title': '埃利斯岛', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['圣地与宗教景点']&#125;&#123;'title': '切尔西市场', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': []&#125;&#123;'title': '商人之家博物馆', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['公园']&#125;&#123;'title': '哈德逊河公园', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': []&#125;&#123;'title': '艾利斯岛移民博物馆', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['剧院']&#125;&#123;'title': '犹太遗产博物馆', 'image': 'https://cc.ddcdn.com/img2/x.gif', 'cate': ['历史博物馆', ',', '景点与地标', ',', '古迹']&#125;。。。。。。 下期说明哈哈，今天总的来说还是比较简单的，容易操作，下期我们准备爬点不一样的东西。。。 今天的分享就到这里了，如果你有任何不懂的问题，可以发信息或者留言喽。","categories":[{"name":"spider","slug":"spider","permalink":"http://licheetools.github.io/categories/spider/"}],"tags":[{"name":"爬虫实战","slug":"爬虫实战","permalink":"http://licheetools.github.io/tags/爬虫实战/"}]},{"title":"爬虫实战系列：（一）58同城-转转商品信息爬取","slug":"爬虫实战系列：（一）58同城-转转商品信息爬取","date":"2017-03-02T02:02:41.000Z","updated":"2019-04-13T07:17:03.001Z","comments":true,"path":"2017/03/02/爬虫实战系列：（一）58同城-转转商品信息爬取/","link":"","permalink":"http://licheetools.github.io/2017/03/02/爬虫实战系列：（一）58同城-转转商品信息爬取/","excerpt":"","text":"说在前面本系列教程记录了博主的爬虫之路，可谓是有苦又累，有甜有笑，如果你在看过本系列教程之后，对爬虫有独到的见解或者浓厚的兴趣，不妨留言和博主进行交流学习呢，我在这里等你来玩~~~ 爬取内容如下图所示，我们需要爬取该图中所示商品的信息，具体包括：类目，标题，价格，浏览量，所在区域等信息。 代码内容字典属性法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#！/user/bin/pytthon# -*- coding:utf-8 -*-# @Time: 2017/4/10 15:15# @Author: lichexo# @File: zhuanzhuan.py# @Website：http：//licheetools.topfrom bs4 import BeautifulSoup #导入需要的包和模块import requestsimport time #用于控制每次爬取的时间间隔user_agent='Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1'headers=&#123; 'User_Agent':'user_agent'&#125;time.sleep(4)def get_attrition(url): #定义函数，便于调用 #url = 'http://zhuanzhuan.58.com/detail/967678151307198471z.shtml ?fullCate=5%2C38484%2C23094&amp;fullLocal=1&amp;from=pc&amp;metric=null&amp; PGTID=0d305a36-0000-1792-ee6b-8d43de6b3765&amp;ClickID=7' #此处URL前面的(#)不可删除，否则就出现函数调用错误，因为后面调用的link已经替换此处的url web_data=requests.get(url,headers=headers) soup=BeautifulSoup(web_data.text,'lxml') casses=soup.select('#nav &gt; div &gt; span &gt; a') titles=soup.select('div.box_left_top &gt; h1') moneys=soup.select('div.price_li &gt; span[class=\"price_now\"] &gt; i') addresses=soup.select('div.palce_li &gt; span &gt; i') for cass,title,money,address in zip(casses,titles,moneys,addresses): data=&#123; 'cass':cass.get_text(), #文字调用get_text()方法 'title':title.get_text(), 'money':money.get_text(), 'address':address.get_text(), &#125; print(data)def get_all_item_info(): url='http://bj.58.com/pbdn/' web_data=requests.get(url) soup=BeautifulSoup(web_data.text,'lxml') href_list=soup.select('a.t') for href in href_list: link=href.get('href') if 'zhuanzhuan' in link: #除去网页中不需要的精准推广部分 get_attrition(link)get_all_item_info() 列表索引法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#！/user/bin/pytthon# -*- coding:utf-8 -*-# @Time: 2017/4/10 15:15# @Author: lichexo# @File: zhuanzhuan.py# @Website：http：//licheetools.topfrom bs4 import BeautifulSoupimport requestsimport timeuser_agent='Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X)AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1'headers=&#123; 'User_Agent':'user_agent'&#125;time.sleep(4)def get_item_info(url): #url='http://zhuanzhuan.58.com/detail/835458751461588996z.shtml ?fullCate=5%2C38484%2C23094&amp;fullLocal=1&amp;from=pc&amp; metric=null&amp;PGTID=0d305a36-0000-123c-630d-da6c51789948&amp;ClickID=1' web_data=requests.get(url,headers=headers) soup=BeautifulSoup(web_data.text,'lxml') title_list=soup.select('h1.info_titile') title=title_list[0].text #该列表有且仅有唯一一个元素，采用get_text()将其取出 price_list=soup.select('span.price_now &gt; i') price=price_list[0].text views_list=soup.select('span.look_time') view=views_list[0].text area_list=soup.select('div.palce_li &gt; span &gt; i') area=area_list[0].text cate_list=soup.select('span.crb_i &gt; a') cate=cate_list[-1].text.strip() #使用strip()函数来消除换行和空格的影响 data=&#123; 'title':title, 'price':price, 'view':view, 'area':area, 'cate':cate, &#125; print(data)def get_all_item_info(): url='http://bj.58.com/pbdn/' web_data=requests.get(url,headers=headers) soup=BeautifulSoup(web_data.text,'lxml') href_list=soup.select('a.t') for href in href_list: link=href.get('href') #获取元素的某个属性采用get()方法来实现 if 'zhuanzhuan' in link: get_item_info(link)get_all_item_info() 运行结果 知识补充soup.select ( )选择标签问题你只需要在浏览器中鼠标右键点击审查(Ctrl+Shift+I) –&gt; 找到字段的具体代码 –&gt; 右键选择Copy –&gt; Copy selector即可得到你想要的代码字段。为保证你是否正确捕抓到相关信息，你可以鼠标右键点击查看网页源代码(Ctrl+U) –&gt;然后再按Ctrl+F调出搜索框 –&gt; 将你刚才捕抓到的信息进行复制黏贴并回车 –&gt; 看右侧的黄色代码区是否是包括你所需信息的最小高亮区，不是则往上一级父级标签查询。 get_text 和 get()的使用问题get_text()，是返回选择的标签文本，具体可以查看参考文档;而get(‘’)，这是选择标签中的属性(也就是里面有=符号的左边)，比如在&lt;li&gt;&lt;a class=&quot;rrence intnal&quot; href=&quot;#next-siblings-previous-siblings&quot;&gt;&lt;/li&gt;之中，选择li &gt; a这个标签之后，则可以用get(‘href’)获取其中的链接。 下期说明哈哈，今天的内容是不是很简单，不过你是不是有一个疑问，就是如果某个内容从属于多个分类(即它有多个标签)我们应该如何把它所属的全部标签都爬取下来呢？那么下期博主将教你如何实现这一功能。 本教程适合有一部分Python基础的同学，小白童鞋很抱歉。。。 今天的分享就到这里了，如果你有任何不懂的问题，可以发信息或者留言喽。","categories":[{"name":"spider","slug":"spider","permalink":"http://licheetools.github.io/categories/spider/"}],"tags":[{"name":"爬虫实战","slug":"爬虫实战","permalink":"http://licheetools.github.io/tags/爬虫实战/"}]}]}